# -*- coding: utf-8 -*-
"""sentiment7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DiVudT4rjHgzbJGApS04SMt7siSfRP1F
"""

!pip install ktrain

!pip install yahoo-fin

!pip install requests_html

from urllib.request import urlopen, Request
from bs4 import BeautifulSoup
import datetime
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import yahoo_fin.stock_info as si
from yahoo_fin import options

import tensorflow as tf
import ktrain
from ktrain import text

data_train = pd.read_csv('/content/trainfin.csv',encoding='latin-1')
data_train.head()

data_test = pd.read_csv('/content/testfin.csv',encoding='latin-1')
data_test.head()

(X_train, y_train), (X_test, y_test), preprocess = text.texts_from_df(train_df=data_train, 
                   text_column='Reviews', label_columns='Sentiment', 
                   val_df=data_test, 
                   maxlen=400, 
                   preprocess_mode='bert')

X_train[0].shape

model = text.text_classifier(name='bert', train_data=(X_train, y_train), 
                             preproc=preprocess)

learner = ktrain.get_learner(model=model, 
                             train_data=(X_train, y_train), 
                             val_data=(X_test, y_test), 
                             batch_size=6)

learner.fit_onecycle(lr=2e-5, epochs=3)

learner

predictor = ktrain.get_predictor(model=learner.model, preproc=preprocess)

testing_data = [ 'With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability .',
                  'The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility  contrary to earlier layoffs the company contracted the ranks of its office workers  the daily positive times reported '
                ]

predictor.predict(texts=testing_data)

predictor.predict(texts=testing_data, return_proba=True)

website_url = 'https://finviz.com/quote.ashx?t='
company_tikcers = ['TTM']

news_tables = {}

for ticker in company_tikcers:
  url = website_url + ticker

  req = Request(url=url, headers={'user-agent': 'my-scrape'})
  response = urlopen(req)
  html = BeautifulSoup(response, 'html')
  news_data = html.find(id='news-table')
  news_tables[ticker] = news_data

parsed_data = []

for ticker, news_table in news_tables.items():
  for row in news_table.findAll('tr'):
    title = row.a.text
    date_data = row.td.text.split(' ')

    if len(date_data) == 1:
      time = date_data[0][0:7]
    else:
      date = datetime.datetime.strptime(date_data[0], '%b-%d-%y').strftime('%Y/%m/%d')
      time = date_data[1][0:7]

    parsed_data.append([ticker, date, time, title])

dataset = pd.DataFrame(parsed_data, columns=["Company", "Date", "Time", "News Headline"])
dataset.to_csv('CompanyNewsHeadlines_WithoutSentiment.csv', index=False)
dataset.head()

dataset['Sentiment'] = dataset['News Headline'].apply(lambda headline: predictor.predict(texts=headline))

dataset

dataset['Date'] = pd.to_datetime(dataset.Date).dt.date

dataset.to_csv('CompanyNewsHeadlines.csv', index=False)

max_min_date = dataset.groupby(['Company']).agg({'Date': [np.min,np.max]})
max_min_date

dataset['MaxDate'] = dataset.groupby('Company').Date.transform('max')
dataset['MinDate'] = dataset.groupby('Company').Date.transform('min')

dataset.head()

company_early_late_dates = {}

for index, row in dataset.iterrows():
  if row['Company'] in company_early_late_dates:
      company_early_late_dates[row['Company']]['early'] = row['MinDate']
      company_early_late_dates[row['Company']]['late'] = row['MaxDate']
  else:
    company_early_late_dates[row['Company']] = {'early': None, 'late': None}

company_early_late_dates

for key, value in company_early_late_dates.items():
  print(key, '->', value)
  tmp_df = si.get_data(key, start_date=value['early'], end_date=value['late'])
  tmp_df['date'] = tmp_df.index
  tmp_df.to_csv(key + '_BERT.csv', index=False)

sentiment_result = {}
for value in dataset['Sentiment']:
  if value in sentiment_result:
      if value == 'positive':
          sentiment_result['positive'] += 1
      elif value == 'negative':
           sentiment_result['negative'] += 1
  else:
    sentiment_result[value] = 1

sentiment_result

result_by_companies = {}
for index, row in dataset.iterrows():
  if row['Company'] in result_by_companies:
    if row['Sentiment'] == 'positive':
      result_by_companies[row['Company']]['positive'] +=1
    if row['Sentiment'] == 'negative':
      result_by_companies[row['Company']]['negative'] +=1
  else:
    result_by_companies[row['Company']] = {'negative': 0,  'positive': 0}

result_by_companies

dataframe_list = []
for key, value in result_by_companies.items():
  print(key, '->', value)
  dataframe_list.append([key, value['negative'], value['positive']])


dataset_company = pd.DataFrame(dataframe_list, columns=["Company", "Negative","Positive"])
dataset_company.head()

index = np.arange(len(company_tikcers))
score_label = np.arange(0, 110, 10)

bar_width = 0.35

fig, ax = plt.subplots()
barNeg = ax.bar(index - bar_width/2, dataset_company['Negative'], bar_width, label='Negative')
barPos = ax.bar(index + bar_width/2, dataset_company['Positive'], bar_width, label='Positive')

ax.set_xticks(index)
ax.set_xticklabels(company_tikcers)

ax.set_yticks(score_label)
ax.set_yticklabels(score_label)


ax.legend()

def insert_data_labels(bars):
  for bar in bars:
    bar_height = bar.get_height()
    ax.annotate('{0:.0f}'.format(bar.get_height()),
                xy=(bar.get_x() + bar.get_width() / 2, bar_height),
                xytext=(0,3),
                textcoords='offset points',
                ha='center',
                va='bottom'
    )

insert_data_labels(barNeg)
insert_data_labels(barPos)

plt.show()

def percentage(part, whole):
  temp = 100 * float(part) / float(whole)
  return format(temp, '.2f')

def visualization(positive, negative):
  labels = ['Positive [' + str(positive) + '%]',
            'Negative [' + str(negative) + '%]']
  sizes = [positive, negative]
  colors = ['lightgreen', 'red']
  patches, texts = plt.pie(sizes, colors=colors, startangle=90)
  plt.title('Aggregate sentiment value of the economic news headlines')
  plt.legend(patches, labels, loc="best")
  plt.axis('equal')
  plt.tight_layout()
  plt.show()

all_num = sentiment_result['positive'] + sentiment_result['negative']
pos_percent = percentage(part=sentiment_result['positive'], whole=all_num)
neg_percent = percentage(part=sentiment_result['negative'], whole=all_num)

visualization(positive=pos_percent,negative=neg_percent)